{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![Lyrics Genius Logo](LyricsGenius.png)\n",
    "\n",
    "Notebook used to train Lyrics Genius given a lyrics dataset and the network specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# Data manipulation\n",
    "import pydot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Misc libraries\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import io\n",
    "\n",
    "\n",
    "# Deep Learning libraries\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, Embedding, InputLayer\n",
    "from keras.layers import LSTM, Lambda, concatenate, Bidirectional, Concatenate, SpatialDropout1D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "from keras.layers.merge import add\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "corpus length: 417951\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading text data...\")\n",
    "text = io.open('data/rhcp-lyrics.txt', encoding='utf-8').read().lower()\n",
    "print('corpus length:', len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters in the corpus: 54\n"
     ]
    }
   ],
   "source": [
    "Tx = 40\n",
    "chars = sorted(list(set(text)))\n",
    "num_classes = len(chars)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print('number of unique characters in the corpus:', len(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(text, Tx = 40, stride = 3):\n",
    "    \"\"\"\n",
    "    Create a training set by scanning a window of size Tx over the text corpus, with stride 3.\n",
    "    \n",
    "    Arguments:\n",
    "    text -- string, corpus of Shakespearian poem\n",
    "    Tx -- sequence length, number of time-steps (or characters) in one training example\n",
    "    stride -- how much the window shifts itself while scanning\n",
    "    \n",
    "    Returns:\n",
    "    X -- list of training examples\n",
    "    Y -- list of training labels\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(0, len(text) - Tx, stride):\n",
    "        X.append(text[i: i + Tx])\n",
    "        Y.append(text[i + Tx])\n",
    "\n",
    "    \n",
    "    print('number of training examples:', len(X))\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Set and Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training set...\n",
      "number of training examples: 139304\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating training set...\")\n",
    "X, Y = build_data(text, Tx=Tx, stride = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(X)\n",
    "# If we already have a character list, then replace the tk.word_index\n",
    "# If not, just skip below part\n",
    "\n",
    "\n",
    "# construct a new vocabulary\n",
    "alphabet = chars\n",
    "\n",
    "\n",
    "#Store alphabet to make predictions\n",
    "with open('models/rhcp-alphabet.json', 'w+') as fp:\n",
    "    json.dump(alphabet, fp)\n",
    "    \n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "\n",
    "# Use char_dict to replace the tk.word_index\n",
    "tk.word_index = char_dict.copy()\n",
    "# Add 'UNK' to the vocabulary\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "\n",
    "\n",
    " \n",
    "with open('models/rhcp-tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tk, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Convert string to index\n",
    "train_sequences = tk.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "train_data = pad_sequences(train_sequences, maxlen=Tx, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "train_data = np.array(train_data, dtype='float32')\n",
    "\n",
    "# =======================Get classes================\n",
    "train_classes = [elem[0] for elem in tk.texts_to_sequences(Y)]\n",
    "\n",
    "train_class_list = [x - 1 for x in train_classes]\n",
    "\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_classes = to_categorical(train_class_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_data, train_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'rnn_width': 64,\n",
    "    'rnn_depth': 4,\n",
    "    'rnn_dropout': 0.3,\n",
    "    'bidirectional': True\n",
    "}\n",
    "embedding_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_learning = False\n",
    "model_path = \"models/rhcp_model_res.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_lstm_cell(rnn_width, rnn_dropout, bidirectional=True, return_sequences=False):\n",
    "    if bidirectional:\n",
    "        return Bidirectional(LSTM(rnn_width, recurrent_dropout=rnn_dropout, dropout=rnn_dropout,return_sequences=return_sequences))\n",
    "    else:\n",
    "        return LSTM(rnn_width, recurrent_dropout=rnn_dropout, dropout=rnn_dropout,return_sequences=return_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_layers(input, rnn_width, rnn_depth, rnn_dropout, bidirectional=True):\n",
    "    layer_list = []\n",
    "    layer = input\n",
    "    for i in range(rnn_depth):\n",
    "        return_sequences = i < rnn_depth - 1\n",
    "        prev_layer = input if i == 0 else layer_list[-1]\n",
    "        layer = new_lstm_cell(rnn_width, rnn_dropout, bidirectional=bidirectional, return_sequences=return_sequences)\n",
    "        \n",
    "        layer_list.append(layer)\n",
    "    return layer, layer_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_residual_lstm_layers(input, rnn_width, rnn_depth, rnn_dropout, bidirectional=True):\n",
    "    \"\"\"\n",
    "    The intermediate LSTM layers return sequences, while the last returns a single element.\n",
    "    The input is also a sequence. In order to match the shape of input and output of the LSTM\n",
    "    to sum them we can do it only for all layers but the last.\n",
    "    \"\"\"\n",
    "    x = input\n",
    "    layer_list = []\n",
    "    for i in range(rnn_depth):\n",
    "        return_sequences = i < rnn_depth - 1\n",
    "        x_rnn = Bidirectional(LSTM(rnn_width, recurrent_dropout=rnn_dropout, dropout=rnn_dropout, return_sequences=return_sequences))(x)\n",
    "        if return_sequences:\n",
    "            # Intermediate layers return sequences, input is also a sequence.\n",
    "            if i > 0 or input.shape[-1] == rnn_width:\n",
    "                x = add([x, x_rnn])\n",
    "            else:\n",
    "                # Note that the input size and RNN output has to match, due to the sum operation.\n",
    "                # If we want different rnn_width, we'd have to perform the sum from layer 2 on.\n",
    "                x = x_rnn\n",
    "        else:\n",
    "            # Last layer does not return sequences, just the last element\n",
    "            # so we select only the last element of the previous output.\n",
    "            def slice_last(x):\n",
    "                return x[..., -1, :]\n",
    "            x = add([Lambda(slice_last)(x), x_rnn])\n",
    "        layer_list.append(x_rnn)\n",
    "    return x, layer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_residual(model_config):\n",
    "    inputs = Input(shape=(Tx, ), name='sent_input', dtype='int64')\n",
    "    \n",
    "    embeddings = keras.layers.Embedding(len(chars) + 1, embedding_size, input_length=Tx)(inputs)\n",
    "    embeddings = SpatialDropout1D(model_config['rnn_dropout'], name='spatial-dropout')(embeddings)\n",
    "    lstm_layer, layer_list = make_residual_lstm_layers(embeddings, **model_config)\n",
    "    \n",
    "    dense_layer = keras.layers.Dense(len(chars), activation='softmax')(lstm_layer)\n",
    "    model = keras.Model(inputs=inputs, outputs=dense_layer)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=4e-3)\n",
    "    model.compile( loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Deep LSTM Model without Residual Units\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(Tx, len(chars))))\n",
    "    model.add(LSTM(128, input_shape=(Tx, len(chars)), return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=4e-3)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "if continue_learning:\n",
    "    model = load_model(model_path)\n",
    "else:\n",
    "    model = create_model_residual(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sent_input (InputLayer)         (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40, 128)      7040        sent_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial-dropout (SpatialDropout (None, 40, 128)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 40, 128)      98816       spatial-dropout[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 40, 128)      98816       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 40, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 40, 128)      98816       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 40, 128)      0           add_1[0][0]                      \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 128)          0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 128)          98816       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 128)          0           lambda_1[0][0]                   \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 54)           6966        add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 409,270\n",
      "Trainable params: 409,270\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    out = np.random.choice(range(len(chars)), p = probas.ravel())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipesulser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "139304/139304 [==============================] - 514s 4ms/step - loss: 2.1137\n",
      "Epoch 2/30\n",
      "139304/139304 [==============================] - 611s 4ms/step - loss: 1.7708\n",
      "Epoch 3/30\n",
      "100480/139304 [====================>.........] - ETA: 3:03 - loss: 1.6911"
     ]
    }
   ],
   "source": [
    "history = model.fit(x, y, batch_size=128, epochs=30, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    x = range(1, len(loss) + 1)\n",
    "\n",
    "\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "model.save(\"models/rhcp_model_res.h5\", overwrite=True)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(temperature=1.0):\n",
    "    generated = ''\n",
    "    usr_input = input(\"Start typing the beginning of your lyrics. Lyric-genius will complete it.\\n Your input is: \")\n",
    "    # zero pad the sentence to Tx characters.\n",
    "    sentence = ('{0:0>' + str(Tx) + '}').format(usr_input).lower()\n",
    "    generated += usr_input \n",
    "\n",
    "    sys.stdout.write(\"\\n\\nHere is your lyric: \\n\\n\") \n",
    "    sys.stdout.write(usr_input)\n",
    "    for i in range(300):\n",
    "\n",
    "        predict_sequence = tk.texts_to_sequences([sentence])\n",
    "\n",
    "        # Padding\n",
    "        predict_data = pad_sequences(predict_sequence, maxlen=Tx, padding='post')\n",
    "\n",
    "        # Convert to numpy array\n",
    "        x_pred = np.array(predict_data, dtype='float32')\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature = temperature)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if next_char == '\\n':\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_output(temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
